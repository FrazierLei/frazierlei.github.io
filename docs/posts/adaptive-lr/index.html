<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Feifei">
    <meta name="description" content="https://feifeizaici.fun">
    <meta name="keywords" content="blog, murmur">

    <meta property="og:site_name" content="éœéœ">
    <meta property="og:title" content="
  é›†ä¸­è‡ªé€‚åº”çš„å­¦ä¹ ç‡ç®—æ³• - éœéœ
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://feifeizaici.fun/posts/adaptive-lr/">
    <meta property="og:image" content="https://feifeizaici.funimages/tn.png">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="https://feifeizaici.fun/posts/adaptive-lr/">
    <meta name="twitter:image" content="https://feifeizaici.funimages/tn.png">

    <base href="https://feifeizaici.fun/posts/adaptive-lr/">
    <title>
  é›†ä¸­è‡ªé€‚åº”çš„å­¦ä¹ ç‡ç®—æ³• - éœéœ
</title>

    <link rel="canonical" href="https://feifeizaici.fun/posts/adaptive-lr/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="../../css/normalize.min.css">
    <link rel="stylesheet" href="../../css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="../../images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="../../images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="https://feifeizaici.fun/index.xml" type="application/rss+xml" title="éœéœ">
      <link href="https://feifeizaici.fun/index.xml" rel="feed" type="application/rss+xml" title="éœéœ" />
    

    <meta name="generator" content="Hugo 0.86.0" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="../../">éœéœ</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://feifeizaici.fun/posts">Blog</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://feifeizaici.fun/about">About</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>






      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="title">é›†ä¸­è‡ªé€‚åº”çš„å­¦ä¹ ç‡ç®—æ³•</h1>
      <h2 class="date">February 2, 2020</h2>
    </header>

    <p>å› ä¸ºä¼—æ‰€ä¸çŸ¥çš„åŸå› ï¼Œè¿™ä¸€å‘¨éƒ½æµ‘æµ‘å™©å™©çš„ï¼Œå¯ä»¥è¯´ä»€ä¹ˆä¹Ÿæ²¡å¹²ï¼Œå¸Œæœ›ä¸‹å‘¨çŠ¶æ€å¥½ä¸€ç‚¹ï¼Œé”»ç‚¼é”»ç‚¼ã€‚
éšä¾¿å†™å†™ï¼Œå½“ä½œä¸€ä¸ªç®€å•çš„ç¬”è®°ï¼Œä¸æ¶‰åŠæ•°å­¦ï¼Œå¤´ç–¼ã€‚</p>
<h2 id="1-sgd-vs-ada">1. SGD v.s. Ada.</h2>
<p>ç›®å‰åº”ç”¨æœ€å¹¿æ³›çš„ä¼˜åŒ–å™¨è¿˜æ˜¯<code>SGD/SGDM</code>å’Œ<code>Adam</code>ï¼Œ<code>SGD</code>å¼ºåœ¨å®ƒæœ€ç»ˆæ”¶æ•›çš„æ•ˆæœï¼Œè€Œä¸€ç³»åˆ—è‡ªé€‚åº”ç®—æ³•å¼ºåœ¨å¯ä»¥åœ¨è®­ç»ƒåˆæœŸå¿«é€Ÿæ”¶æ•›ã€‚ä¸‹é¢è¿™ä¸ªå›¾å¯ä»¥è¯´æ˜è¿™ç§æƒ…å†µ
<img src="https://qiniusave.feifeizaici.xyz/Fg1Ndo3O3wjfAU95D4bU93FFG3gl" alt=""></p>
<h2 id="2å¸¸è§çš„è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•">2.å¸¸è§çš„è‡ªé€‚åº”å­¦ä¹ ç‡ç®—æ³•</h2>
<!-- raw HTML omitted -->
<h3 id="21-adagrad">2.1 AdaGrad</h3>
<p><img src="https://qiniusave.feifeizaici.xyz/FtfPtXKt9v8mxFwpCW2ThjSJfTSN" alt="">
<code>AdaGrad</code>çš„æƒ³æ³•æ˜¯å…·æœ‰è¾ƒå¤§åå¯¼çš„å‚æ•°åº”è¯¥æœ‰ä¸€ä¸ªè¾ƒå¤§çš„å­¦ä¹ ç‡ï¼Œäºæ˜¯ç”¨æ¯ä¸€ä¸ªå‚æ•°çš„æ‰€æœ‰æ¢¯åº¦çš„å†å²å¹³æ–¹å€¼å¾—å’Œçš„å¹³æ–¹æ ¹æ¥ç¼©æ”¾ã€‚</p>
<h3 id="22-rmsprop">2.2 RMSProp</h3>
<p><img src="https://qiniusave.feifeizaici.xyz/Fu_qCB7CEu7jbMlQRJerXCgonfzb" alt="">
<code>RMSProp</code>æ˜¯åœ¨<code>AdaGrad</code>çš„åŸºç¡€ä¸Šæ”¹åŠ¨çš„ï¼Œæ€è·¯æ˜¯ä¸¢å¼ƒé¥è¿œè¿‡å»çš„å†å²ï¼Œè®©æœ€è¿‘å‡ æ¬¡æ¢¯åº¦çš„æƒé‡å¤§ä¸€äº›ã€‚</p>
<h3 id="23-adam">2.3 Adam</h3>
<p><img src="https://qiniusave.feifeizaici.xyz/FrJkwbhBlmsnBdxGDisnnMgstpO0" alt="">
<code>Adam</code>ç®—æ³•å¯ä»¥çœ‹ä½œæ˜¯<code>RMSProp</code>+<code>momentum</code>
ç”¨ä¿®æ­£åçš„æœ‰åä¸€é˜¶çŸ©ä»£æ›¿æ¢¯åº¦ï¼Œç”¨ä¿®æ­£åçš„æœ‰åäºŒé˜¶çŸ©çš„å¹³æ–¹æ ¹ä½œä¸ºç¼©æ”¾çš„æ¯”ä¾‹ã€‚
<code>Adam</code>é€šå¸¸è¢«è®¤ä¸ºå¯¹è¶…å‚æ•°çš„é€‰æ‹©ç›¸å½“é²æ£’ï¼Œé™¤äº†ä½œè€…æ¨èçš„<code>1e-4</code>ï¼Œ<code>3e-4</code>å’Œ<code>5e-4</code>ä¹Ÿç»å¸¸è¢«é€‰æ‹©ä½œä¸ºåˆå§‹å­¦ä¹ ç‡ã€‚ğŸ¶
<img src="https://qiniusave.feifeizaici.xyz/FvJVBoTGKBNfVanuAyHSmAJhNDfI" alt=""></p>
<h4 id="pytorch-å®ç°">PyTorch å®ç°</h4>
<p><code>Adam</code>çš„<a href="https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam">æºç </a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
<span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>

<span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

<span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">bias_correction1</span>
<span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="o">-</span><span class="n">step_size</span><span class="p">,</span> <span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
</code></pre></div><p>è¿™é‡Œå¯ä»¥æŠŠè®¡ç®—çœ‹åšæ˜¯åˆ†æˆä¸¤æ­¥</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">\text{denom}=\sqrt{\frac{v_t}{1-beta_2^t}}+\epsilon\\
\Delta x=-\frac{\alpha_t}{1-\beta_1^t} \cdot \frac{m_t}{\text{denom}}=-\frac{\alpha_t}{1-\beta_1^t} \cdot \frac{m_t}{\sqrt{\frac{v_t}{1-\beta_2^t}}+\epsilon}
</code></pre></div><h3 id="24-adabound">2.4 Adabound</h3>
<p><a href="https://arxiv.org/pdf/1902.09843.pdf">ADAPTIVE GRADIENT METHODS WITH DYNAMIC
BOUND OF LEARNING RATE</a>
äºŒä½œæ˜¯å¯çˆ±çš„ğŸ»ç¥ï¼Œè¿™ç¯‡è®ºæ–‡åæ¥è¢«æŒ‡å‡ºæ¥æœ‰<a href="https://arxiv.org/pdf/1908.04457.pdf">æ•°å­¦é”™è¯¯</a>ï¼Œä½†æ˜¯ç”¨æˆ‘è´«ç˜ çš„æ•°å­¦çŸ¥è¯†åœ¨çŸ­æ—¶é—´å†…æ˜¯ä¸å¤ªå¯èƒ½çœ‹æ‡‚çš„ï¼Œä¹Ÿå°±ä¸çœ‹äº†ï¼Œæƒ³åˆ°è¿™é‡Œè¿˜çœŸæ˜¯éš¾å—ï¼Œä¸ºä»€ä¹ˆè‡ªå·±å°±è¿™ä¹ˆèœå•Šï¼Œå‘å¾®ã€‚
å‘å¾®æ˜¯è¦å‘å¾®çš„ï¼Œå‘å¾®å®Œäº†çœ‹çœ‹äººå®¶æ˜¯æ€ä¹ˆå®ç°çš„ï¼Œçœ‹æ‡‚æ•°å­¦æ¨å¯¼å¾ˆéš¾ï¼Œçœ‹æ‡‚ç®—æ³•æµç¨‹å’Œå®ç°æ–¹æ³•è¿˜æ˜¯å¯è¡Œçš„ã€‚
<img src="https://qiniusave.feifeizaici.xyz/FhX4WpQm2bvK-MWefcMjaiSSqxJK" alt="">
æ–‡ç« ä¸ºäº†å’Œå¼•ç”¨çš„æ–‡ç« æ‰€ç”¨ç¬¦å·åŒ¹é…ä¸Šï¼Œç”¨äº†ä¸€äº›èŠ±é‡Œèƒ¡å“¨çš„è¡¨è¾¾ï¼Œå…¶å®æ ¸å¿ƒæ€æƒ³å¾ˆç®€å•:</p>
<ul>
<li>$\alpha$æ˜¯åˆå§‹æ­¥é•¿</li>
<li>$\beta$æ˜¯çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡é€Ÿç‡</li>
<li>$g_t$æ˜¯æ¢¯åº¦</li>
<li>$m_t$æ˜¯ä¸€é˜¶çŸ©</li>
<li>$v_t$æ˜¯äºŒé˜¶çŸ©</li>
<li>éšæ—¶é—´å˜åŒ–çš„å­¦ä¹ ç‡$\eta_t$è¦ç»è¿‡ä¸‹ç•Œ$\eta_{l}(t)$å’Œä¸Šç•Œ$\eta_{u}(t)$çš„æˆªæ–­ï¼ŒåŒæ—¶è¿˜æœ‰ä¸€ä¸ª$\sqrt{t}$çš„è¡°å‡</li>
</ul>
<p>æ–‡ç« åœ¨å®éªŒéƒ¨åˆ†é‡‡ç”¨çš„ä¸‹ç•Œ$\eta_{l}(t)$å’Œä¸Šç•Œ$\eta_{u}(t)$åˆ†åˆ«æ˜¯</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">\begin{aligned}
\eta_{l}(t)&amp;=0.1-\frac{0.1}{\left(1-\beta_{2}\right) t+1}\\
\eta_{u}(t)&amp;=0.1+\frac{0.1}{\left(1-\beta_{2}\right) t}
\end{aligned}
</code></pre></div><h4 id="pytorch-å®ç°-1">PyTorch å®ç°</h4>
<p><a href="https://github.com/Luolc/AdaBound">ä»£ç </a>æ˜¯åœ¨PyTorchçš„<code>Adam</code>çš„<a href="https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam">æºç </a>åŸºç¡€ä¸Šä¿®æ”¹çš„</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

<span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

<span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
<span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)</span> <span class="o">/</span> <span class="n">bias_correction1</span>

<span class="n">lower_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">upper_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]))</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">denom</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
<span class="n">step_size</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">denom</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">)</span>

<span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">step_size</span><span class="p">)</span>
</code></pre></div><p>è¿™é‡Œä¹Ÿå¯ä»¥æŠŠè®¡ç®—çœ‹åšæ˜¯åˆ†æˆä¸¤æ­¥</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">\text{denom}=\sqrt{v_t}+\epsilon\\
\Delta x=-\left\lfloor \frac{\alpha_t\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}}{\text{denom}} \right\rceil _{\eta_l(t),\eta_u(t)} \cdot m_t=-\left\lfloor \frac{\alpha_t\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}}{\sqrt{v_t}+\epsilon} \right\rceil _{\eta_l(t),\eta_u(t)} \cdot m_t
</code></pre></div><p>æˆ‘ç”¨ç¬¦å·$\left\lfloor \right\rceil_{\eta_l(t),\eta_l(t)}$è¡¨ç¤ºç”¨ä¸‹ç•Œ$\eta_{l}(t)$å’Œä¸Šç•Œ$\eta_{u}(t)$è¿›è¡Œæˆªæ–­ã€‚</p>
<h3 id="26-adabid">2.6 AdaBID</h3>
<p>æˆ‘ä¸ºäº†å®Œæˆä¼˜åŒ–å¤§ä½œä¸šè‡ªå·±ç¼–çš„
<img src="https://qiniusave.feifeizaici.xyz/FrOEriX1NYznm9a8WjkgRtV007rE" alt="">
<code>PyTorch</code>å®ç°ï¼š</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>

<span class="k">class</span> <span class="nc">AdaBID</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Implements AdaBID algorithm.
</span><span class="s2">    Arguments:
</span><span class="s2">        params (iterable): iterable of parameters to optimize or dicts defining
</span><span class="s2">            parameter groups
</span><span class="s2">        lr (float, optional): Adam learning rate (default: 1e-3)
</span><span class="s2">        beta1 (float, optional): coefficients used for computing
</span><span class="s2">            running averages of gradient (default: 0.9)
</span><span class="s2">        final_lr (float, optional): final (SGD) learning rate (default: 0.1)
</span><span class="s2">        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)
</span><span class="s2">        eps (float, optional): term added to the denominator to improve
</span><span class="s2">            numerical stability (default: 1e-8)
</span><span class="s2">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
</span><span class="s2">        c (float, optional): decay rate of beta2 (default: 0.8)
</span><span class="s2">    &#34;&#34;&#34;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">final_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.8</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">beta1</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">final_lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid final learning rate: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">final_lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">gamma</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid gamma parameter: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid c parameter: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="n">beta1</span><span class="p">,</span> <span class="n">final_lr</span><span class="o">=</span><span class="n">final_lr</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBID</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">group</span><span class="p">:</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBID</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="s2">&#34;&#34;&#34;Performs a single optimization step.
</span><span class="s2">        Arguments:
</span><span class="s2">            closure (callable, optional): A closure that reevaluates the model
</span><span class="s2">                and returns the loss.
</span><span class="s2">        &#34;&#34;&#34;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span><span class="p">,</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s1">&#39;Adam does not support sparse gradients, please consider SparseAdam instead&#39;</span><span class="p">)</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="c1"># Exponential moving average of squared gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;beta1&#39;</span><span class="p">]</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="c1"># Update betas</span>
                <span class="n">beta1_correction</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                <span class="n">beta2_correction</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="n">c</span>

                <span class="c1"># Decay the first and second moment running average coefficient</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1_correction</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1_correction</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2_correction</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_correction</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
                <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

                <span class="c1"># Applies bounds on actual learning rate</span>
                <span class="c1"># lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay</span>
                <span class="n">final_lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;final_lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">base_lr</span>
                <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]))</span>
                <span class="n">step_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">denom</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
                <span class="n">step_size</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">denom</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">)</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">step_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>
  </article>

  <br/>

  
  
</section>

      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
        
        
        
        
      </div>
    
    
      <p>Hello, world.</p>
    
     Â© 2021    Â·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 

  </section>
</footer>
<div class="fixed-bar">
  <section class="container">
    
      <p id="privateTriggerText">éœéœåœ¨å“ª<a id="privateTrigger">Click!</a></p>
    
    
      <div class="sns-shares pc-sns-shares">
        
        
        
        
        
      </div>
    
  </section>
</div>

      
    </main>

    

  <script src="../../js/app.js"></script>
  
  <script>
  (function($) {
    $(function() {
      $('#privateTrigger').on('click', function() {
        $('.private').slideToggle();
        $('#privateTriggerText').text("éœéœåœ¨æ­¤");
      });
    });
   })(jQuery);
  </script>
  
  </body>
</html>
