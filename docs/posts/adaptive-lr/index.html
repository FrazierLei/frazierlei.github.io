<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Feifei">
    <meta name="description" content="https://feifeizaici.fun">
    <meta name="keywords" content="blog, murmur">

    <meta property="og:site_name" content="霏霏">
    <meta property="og:title" content="
  集中自适应的学习率算法 - 霏霏
">
    <meta property="og:description" content="">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://feifeizaici.fun/posts/adaptive-lr/">
    <meta property="og:image" content="https://feifeizaici.funimages/tn.png">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="https://feifeizaici.fun/posts/adaptive-lr/">
    <meta name="twitter:image" content="https://feifeizaici.funimages/tn.png">

    <base href="https://feifeizaici.fun/posts/adaptive-lr/">
    <title>
  集中自适应的学习率算法 - 霏霏
</title>

    <link rel="canonical" href="https://feifeizaici.fun/posts/adaptive-lr/">
    
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
    
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="../../css/normalize.min.css">
    <link rel="stylesheet" href="../../css/style.min.css">

    

    

    <link rel="icon" type="image/png" href="../../images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="../../images/favicon-16x16.png" sizes="16x16">

    
      <link rel="alternate" href="https://feifeizaici.fun/index.xml" type="application/rss+xml" title="霏霏">
      <link href="https://feifeizaici.fun/index.xml" rel="feed" type="application/rss+xml" title="霏霏" />
    

    <meta name="generator" content="Hugo 0.86.0" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="../../">霏霏</a>
    <input type="checkbox" id="menu-control"/>
    <label class="menu-mobile  float-right " for="menu-control">
      <span class="btn-mobile  float-right ">&#9776;</span>
      <ul class="navigation-list">
        
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://feifeizaici.fun/posts">Blog</a>
            </li>
          
            <li class="navigation-item  align-center ">
              <a class="navigation-link" href="https://feifeizaici.fun/about">About</a>
            </li>
          
        
        
      </ul>
    </label>
  </section>
</nav>






      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="title">集中自适应的学习率算法</h1>
      <h2 class="date">February 2, 2020</h2>
    </header>

    <p>因为众所不知的原因，这一周都浑浑噩噩的，可以说什么也没干，希望下周状态好一点，锻炼锻炼。
随便写写，当作一个简单的笔记，不涉及数学，头疼。</p>
<h2 id="1-sgd-vs-ada">1. SGD v.s. Ada.</h2>
<p>目前应用最广泛的优化器还是<code>SGD/SGDM</code>和<code>Adam</code>，<code>SGD</code>强在它最终收敛的效果，而一系列自适应算法强在可以在训练初期快速收敛。下面这个图可以说明这种情况
<img src="https://qiniusave.feifeizaici.xyz/Fg1Ndo3O3wjfAU95D4bU93FFG3gl" alt=""></p>
<h2 id="2常见的自适应学习率算法">2.常见的自适应学习率算法</h2>
<!-- raw HTML omitted -->
<h3 id="21-adagrad">2.1 AdaGrad</h3>
<p><img src="https://qiniusave.feifeizaici.xyz/FtfPtXKt9v8mxFwpCW2ThjSJfTSN" alt="">
<code>AdaGrad</code>的想法是具有较大偏导的参数应该有一个较大的学习率，于是用每一个参数的所有梯度的历史平方值得和的平方根来缩放。</p>
<h3 id="22-rmsprop">2.2 RMSProp</h3>
<p><img src="https://qiniusave.feifeizaici.xyz/Fu_qCB7CEu7jbMlQRJerXCgonfzb" alt="">
<code>RMSProp</code>是在<code>AdaGrad</code>的基础上改动的，思路是丢弃遥远过去的历史，让最近几次梯度的权重大一些。</p>
<h3 id="23-adam">2.3 Adam</h3>
<p><img src="https://qiniusave.feifeizaici.xyz/FrJkwbhBlmsnBdxGDisnnMgstpO0" alt="">
<code>Adam</code>算法可以看作是<code>RMSProp</code>+<code>momentum</code>
用修正后的有偏一阶矩代替梯度，用修正后的有偏二阶矩的平方根作为缩放的比例。
<code>Adam</code>通常被认为对超参数的选择相当鲁棒，除了作者推荐的<code>1e-4</code>，<code>3e-4</code>和<code>5e-4</code>也经常被选择作为初始学习率。🐶
<img src="https://qiniusave.feifeizaici.xyz/FvJVBoTGKBNfVanuAyHSmAJhNDfI" alt=""></p>
<h4 id="pytorch-实现">PyTorch 实现</h4>
<p><code>Adam</code>的<a href="https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam">源码</a></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
<span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>

<span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

<span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">bias_correction1</span>
<span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="o">-</span><span class="n">step_size</span><span class="p">,</span> <span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>
</code></pre></div><p>这里可以把计算看做是分成两步</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">\text{denom}=\sqrt{\frac{v_t}{1-beta_2^t}}+\epsilon\\
\Delta x=-\frac{\alpha_t}{1-\beta_1^t} \cdot \frac{m_t}{\text{denom}}=-\frac{\alpha_t}{1-\beta_1^t} \cdot \frac{m_t}{\sqrt{\frac{v_t}{1-\beta_2^t}}+\epsilon}
</code></pre></div><h3 id="24-adabound">2.4 Adabound</h3>
<p><a href="https://arxiv.org/pdf/1902.09843.pdf">ADAPTIVE GRADIENT METHODS WITH DYNAMIC
BOUND OF LEARNING RATE</a>
二作是可爱的🐻神，这篇论文后来被指出来有<a href="https://arxiv.org/pdf/1908.04457.pdf">数学错误</a>，但是用我贫瘠的数学知识在短时间内是不太可能看懂的，也就不看了，想到这里还真是难受，为什么自己就这么菜啊，卑微。
卑微是要卑微的，卑微完了看看人家是怎么实现的，看懂数学推导很难，看懂算法流程和实现方法还是可行的。
<img src="https://qiniusave.feifeizaici.xyz/FhX4WpQm2bvK-MWefcMjaiSSqxJK" alt="">
文章为了和引用的文章所用符号匹配上，用了一些花里胡哨的表达，其实核心思想很简单:</p>
<ul>
<li>$\alpha$是初始步长</li>
<li>$\beta$是矩估计的指数衰减速率</li>
<li>$g_t$是梯度</li>
<li>$m_t$是一阶矩</li>
<li>$v_t$是二阶矩</li>
<li>随时间变化的学习率$\eta_t$要经过下界$\eta_{l}(t)$和上界$\eta_{u}(t)$的截断，同时还有一个$\sqrt{t}$的衰减</li>
</ul>
<p>文章在实验部分采用的下界$\eta_{l}(t)$和上界$\eta_{u}(t)$分别是</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">\begin{aligned}
\eta_{l}(t)&amp;=0.1-\frac{0.1}{\left(1-\beta_{2}\right) t+1}\\
\eta_{u}(t)&amp;=0.1+\frac{0.1}{\left(1-\beta_{2}\right) t}
\end{aligned}
</code></pre></div><h4 id="pytorch-实现-1">PyTorch 实现</h4>
<p><a href="https://github.com/Luolc/AdaBound">代码</a>是在PyTorch的<code>Adam</code>的<a href="https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam">源码</a>基础上修改的</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>

<span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

<span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
<span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)</span> <span class="o">/</span> <span class="n">bias_correction1</span>

<span class="n">lower_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">upper_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]))</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">denom</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
<span class="n">step_size</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">denom</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">)</span>

<span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">step_size</span><span class="p">)</span>
</code></pre></div><p>这里也可以把计算看做是分成两步</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback">\text{denom}=\sqrt{v_t}+\epsilon\\
\Delta x=-\left\lfloor \frac{\alpha_t\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}}{\text{denom}} \right\rceil _{\eta_l(t),\eta_u(t)} \cdot m_t=-\left\lfloor \frac{\alpha_t\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}}{\sqrt{v_t}+\epsilon} \right\rceil _{\eta_l(t),\eta_u(t)} \cdot m_t
</code></pre></div><p>我用符号$\left\lfloor \right\rceil_{\eta_l(t),\eta_l(t)}$表示用下界$\eta_{l}(t)$和上界$\eta_{u}(t)$进行截断。</p>
<h3 id="26-adabid">2.6 AdaBID</h3>
<p>我为了完成优化大作业自己编的
<img src="https://qiniusave.feifeizaici.xyz/FrOEriX1NYznm9a8WjkgRtV007rE" alt="">
<code>PyTorch</code>实现：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>

<span class="k">class</span> <span class="nc">AdaBID</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="s2">&#34;&#34;&#34;Implements AdaBID algorithm.
</span><span class="s2">    Arguments:
</span><span class="s2">        params (iterable): iterable of parameters to optimize or dicts defining
</span><span class="s2">            parameter groups
</span><span class="s2">        lr (float, optional): Adam learning rate (default: 1e-3)
</span><span class="s2">        beta1 (float, optional): coefficients used for computing
</span><span class="s2">            running averages of gradient (default: 0.9)
</span><span class="s2">        final_lr (float, optional): final (SGD) learning rate (default: 0.1)
</span><span class="s2">        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)
</span><span class="s2">        eps (float, optional): term added to the denominator to improve
</span><span class="s2">            numerical stability (default: 1e-8)
</span><span class="s2">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
</span><span class="s2">        c (float, optional): decay rate of beta2 (default: 0.8)
</span><span class="s2">    &#34;&#34;&#34;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">final_lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.8</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">beta1</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">final_lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid final learning rate: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">final_lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">gamma</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid gamma parameter: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&#34;Invalid c parameter: </span><span class="si">{}</span><span class="s2">&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="n">beta1</span><span class="p">,</span> <span class="n">final_lr</span><span class="o">=</span><span class="n">final_lr</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBID</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">group</span><span class="p">:</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBID</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="s2">&#34;&#34;&#34;Performs a single optimization step.
</span><span class="s2">        Arguments:
</span><span class="s2">            closure (callable, optional): A closure that reevaluates the model
</span><span class="s2">                and returns the loss.
</span><span class="s2">        &#34;&#34;&#34;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span><span class="p">,</span> <span class="n">base_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s1">&#39;Adam does not support sparse gradients, please consider SparseAdam instead&#39;</span><span class="p">)</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="c1"># Exponential moving average of squared gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;beta1&#39;</span><span class="p">]</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="c1"># Update betas</span>
                <span class="n">beta1_correction</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                <span class="n">beta2_correction</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="n">c</span>

                <span class="c1"># Decay the first and second moment running average coefficient</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1_correction</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1_correction</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2_correction</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_correction</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
                <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

                <span class="c1"># Applies bounds on actual learning rate</span>
                <span class="c1"># lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay</span>
                <span class="n">final_lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;final_lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">base_lr</span>
                <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">upper_bound</span> <span class="o">=</span> <span class="n">final_lr</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]))</span>
                <span class="n">step_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">denom</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)</span>
                <span class="n">step_size</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">denom</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper_bound</span><span class="p">)</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">)</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">step_size</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>
  </article>

  <br/>

  
  
</section>

      </div>
      
        <footer class="footer">
  <section class="container">
    
      <div class="sns-shares sp-sns-shares">
        
        
        
        
        
      </div>
    
    
      <p>Hello, world.</p>
    
     © 2021    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/naro143/hugo-coder-portfolio">CoderPortfolio</a>. 

  </section>
</footer>
<div class="fixed-bar">
  <section class="container">
    
      <p id="privateTriggerText">霏霏在哪<a id="privateTrigger">Click!</a></p>
    
    
      <div class="sns-shares pc-sns-shares">
        
        
        
        
        
      </div>
    
  </section>
</div>

      
    </main>

    

  <script src="../../js/app.js"></script>
  
  <script>
  (function($) {
    $(function() {
      $('#privateTrigger').on('click', function() {
        $('.private').slideToggle();
        $('#privateTriggerText').text("霏霏在此");
      });
    });
   })(jQuery);
  </script>
  
  </body>
</html>
